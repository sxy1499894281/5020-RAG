```markdown
# PPT 全量讲稿（逐页）  
—— 面向“基于 arXiv 摘要的多策略 RAG 问答系统”

> 用法：  
> - 每一页下面是可以“几乎照读”的讲稿，你可根据时间和习惯删减。  
> - 与实验数据强相关的页（主要是 Slide 9），我先给出 **数据分析步骤** 和 **讲稿模版思路**，等你做完实验再把具体数值填进去。

---

## Slide 1｜标题 & 项目概览

**讲稿示例**

> 各位老师、同学好，我是 XXX，今天要汇报的是我这次课程项目：  
> **《基于 arXiv 摘要的多策略增强 RAG 学术问答系统》**。
>
> 这个系统的目标，可以简单理解为做一个“懂论文的问答助手”。  
> 当用户提出一个学术问题，比如“什么是对比学习？”或者“图神经网络怎么用在文本分类上？”，系统会在 arXiv 的论文摘要中自动检索相关论文，再让大语言模型在这些论文的基础上给出解释，并且标出主要参考了哪些论文、哪些句子。
>
> 整个系统包含两大块：  
> 第一块是一个完整的 RAG 架构，包括数据预处理、BM25 和向量检索、统一检索接口、RAG 生成以及评测模块；  
> 第二块是在这个架构上增加的六个增强模块：动态混合权重、跨编码器重排、查询扩展、证据片段高亮、类别感知检索，以及基于延迟预算的 SLA 策略选择。
>
> 后面的汇报，我会先介绍任务和数据，再讲系统架构，然后依次讲这六个关键方法，最后用实验结果做一个整体总结。

---

## Slide 2｜任务背景与项目目标

**讲稿示例**

> 这一页主要说明项目做什么、为什么要这样做。
>
> 课程的题目是“面向特定领域问答的检索增强生成”，要求我们在一个具体领域的语料库上，搭建一个 RAG 系统：  
> 要用向量数据库做稠密检索，要和大语言模型结合起来做问答，还要在准确率、召回率和延迟这些指标上做系统化评估。
>
> 本项目选择的领域是**学术论文**，具体来说是 arXiv 网站上的论文摘要。  
> 选择 arXiv 的原因有两点：  
> 一是数据公开、规模大，非常适合做检索和实验；  
> 二是很多问题本身就很学术化，比如“某种模型是什么”、“某种方法的优点是什么”，答案天然存在于论文里。
>
> 项目的核心目标是：  
> 1. 搭建一个完整的学术 RAG 系统，覆盖预处理、索引、检索、问答和评测全流程；  
> 2. 在这个系统中设计并实现六种检索和控制层面的增强策略，让系统在复杂问题下表现更好，也更可解释、更可控。

---

## Slide 3｜数据与系统整体架构

**讲稿示例**

> 下面这张图展示了系统的整体架构，从数据到问答，从离线处理到在线查询。
>
> 在数据层面，我们使用 arXiv 提供的 metadata 快照，它是一个大约 4.6GB 的 JSONL 文件，每一行对应一篇论文的完整信息。  
> 通过 `ingest.py` 进行预处理，只保留我们真正需要的字段：论文 id、标题、摘要、学科类别和创建时间，写入一个更小的 `clean.jsonl` 文件，便于后续的索引和检索。
>
> 在索引层面，我们构建了两条检索通路：  
> - 一条是基于 BM25 的关键词检索，负责对关键词非常敏感的问题；  
> - 另一条是基于 `sentence-transformers + Chroma` 的稠密向量检索，负责理解语义相近但措辞不同的问题。  
> 这两条通路的接口统一封装在 `retriever.py` 里，外部可以选择 `bm25`、`dense` 或者 `hybrid` 模式。
>
> 在生成层面，`rag.py` 负责把检索到的论文摘要拼接成上下文，并通过 `LLMClient` 调用大语言模型生成答案，同时带上引用的论文信息。  
> 为了评估系统，我们用 `synth_qa.py` 从 `clean.jsonl` 中自动生成问答数据集，再用 `eval.py` 计算 Recall、MRR 和延迟等指标。
>
> 在这个基础的架构之上，我们引入了六个增强模块，分别放在“query 进入系统、检索、重排、构造上下文”这些关键位置，后面我会一一展开。

---

## Slide 4｜关键方法 1：动态混合权重（自适应 α）

**讲稿示例**

> 第一个关键方法是动态混合权重，也就是让 BM25 和稠密检索之间的权重 alpha 随查询类型自适应变化。
>
> 在 Hybrid 检索中，我们需要用一个 α 来控制：  
> `最终得分 = α * 稠密得分 + (1 - α) * BM25 得分`。  
> 如果 α 固定，就会遇到一个问题：  
> - 对“术语型”查询，比如“BERT 层数”、“ResNet-50 结构”，关键词非常明确，更适合 BM25；  
> - 对“语义型”查询，比如“怎么让大模型更稳定地处理长文本”，更依赖语义理解，适合稠密向量；  
> 固定 α 很难同时兼顾这两种情况。
>
> 为了解决这个问题，`heuristics.py` 中实现了一个轻量的 `classify_query` 函数：  
> 它会分析 query 的长度、数字和符号的比例等特征，把查询粗略分为三类：术语型、语义型和混合型。  
> 然后根据类别给出建议的 α：术语型偏向 BM25，比如 α≈0.2；语义型偏向稠密，比如 α≈0.8；混合型则用 0.5 做折中。
>
> 在增强版检索接口 `retrieve_enhanced` 中，只要配置中打开了 `dynamic_alpha`，并且没有手动指定 α，就会先调用 `classify_query` 得到一个自适应 α，再用这个 α 为当前查询做 Hybrid 融合。  
> 这样可以在几乎不增加计算成本的情况下，让系统对不同风格的问题自动选择更合理的 BM25/向量权重。

---

## Slide 5｜关键方法 2：跨编码器重排（Rerank）

**讲稿示例**

> 第二个关键方法是跨编码器重排，也就是在初次检索之后，再用一个更强的模型对候选结果做精排。
>
> 在基础检索阶段，BM25 和稠密向量采用的是 Bi-Encoder 结构，即分别对 query 和文档编码，再看向量之间的相似度。  
> 这种方式的优点是速度快、适合大规模索引，但它在精细排序上的能力有限，对于 RAG 场景里最重要的前几名结果，有时候排序不够理想。
>
> 为了提升前几名的可靠性，我们在 `reranker.py` 中引入了 Cross-Encoder 模型，比如 `bge-reranker-base`。  
> Cross-Encoder 的思路是：把 query 和某篇文档的文本拼接在一起，让模型“同时读两者”，然后直接输出一个相关性分数。  
> 这种方式更像是让模型逐篇“认真读”，因此对排名前几的质量非常有帮助。
>
> 在系统中，我们不会对所有文档都用 Cross-Encoder，那样速度太慢。  
> 实际做法是：先用 Hybrid 初排得到 Top-N 篇候选，比如前 50 条；然后只对这 50 条调用 `rerank(query, docs, model_name, topk)` 进行重排，最后选出新的 Top-k 作为最终结果。  
> 在实验部分，我们会看到，这种重排方式能显著提高 MRR 和引用质量，但也会带来一定的检索延迟，这是一种典型的“效果与速度的权衡”。

---

## Slide 6｜关键方法 3：查询扩展（PRF + LLM 改写）

**讲稿示例**

> 第三个关键方法是查询扩展（Query Expansion），目标是在不修改用户原始问题的前提下，自动生成更丰富的检索请求，从而提高召回率。
>
> 在真实使用场景中，用户的提问往往比较简短，例如“improve transformer efficiency”或者“contrastive loss for images”，信息非常有限。  
> 有时候用户用的是系统不熟悉的说法，或者省略了关键限定词，这都会导致检索不到应该命中的论文。
>
> 在 `expansion.py` 里，我采用了两种方式来扩展查询：  
> 第一种是 **PRF（伪相关反馈）**：  
> - 先用原始 query 检索一次，取前 M 篇候选文档；  
> - 从这些文档中统计高频关键词，选出一些在当前任务中比较“有信息量”的词；  
> - 把这些词拼回到 query 中，形成一个“加长版”的查询。  
> 第二种是 **LLM 改写**：  
> - 使用 `LLMClient`，给大模型输入原始 query，让它生成几种语义类似但是用词不同、或者更具体的问法。  
>
> 综合这两种方式，`expand_query` 会输出一个 query 变体列表，比如 `[原始问题, 加 PRF 词的问题, LLM 版本 1, LLM 版本 2...]`。  
> 在增强检索时，可以对这些变体并行检索，然后对结果做归一化和去重。  
> 这样一来，系统有更大机会覆盖到原本漏检的论文，特别是在 Recall@10 或 Recall@20 这类指标上会有明显提升。当然，这也会带来额外的检索开销，后面可以配合 SLA 策略进行控制。

---

## Slide 7｜关键方法 4：证据片段高亮（句级 Evidence）

**讲稿示例**

> 第四个关键方法是证据片段高亮，重点解决的是两件事：  
> 一是让模型看到的上下文更干净、更聚焦；  
> 二是让用户清楚地知道“答案背后引用的是哪篇论文、哪一句话”。
>
> 对于每篇命中的论文，我们在 `snippets.py` 里会做两步操作：  
> 第一步，用 `sentence_split` 将论文的 title 和 abstract 切分成若干句子；  
> 第二步，用 `score_sentences` 对每个句子计算与 query 的相关性分数，可以从简单的词重合度做起，也可以升级到基于向量的相似度。  
> 然后通过 `select_evidence_for_docs` 为每篇文档选出若干 Top 句，将它们连同论文 id 和标题一起整理成一个 evidence 列表。
>
> 在 RAG 阶段，增强版的 `enhanced_answer` 会调用 `select_evidence_for_docs` 获取这些证据句。如果配置中指定使用 evidence snippets，就会用 `build_context_with_evidence` 仅基于这些高分句构建上下文，而不是原封不动地拼接整个摘要。  
> 这样一方面能减少噪音、缩短上下文长度，另一方面还能在最终输出中直接展示类似：  
> `[paper_id] paper_title :: 具体句子` 这样的证据列表。
>
> 在展示时，我会选一两个具体的回答示例：先看模型给出的自然语言答案，再往下看 evidence 部分，就能清楚地看到这个答案是“从哪几篇论文的哪几句话推导出来的”，这对信任感和可解释性都非常重要。

---

## Slide 8｜关键方法 5 & 6：类别感知检索 + SLA 策略选择

**讲稿示例**

> 第五个关键方法是类别感知检索，利用 arXiv 自带的学科标签，减少不相关领域的干扰。
>
> arXiv 把论文分成很多学科类别，比如 `cs.CL` 表示计算语言学，`cs.LG` 表示机器学习，`astro-ph` 表示天体物理等等。  
> 当用户问“图神经网络用于文本分类”的问题时，我们期望命中的论文主要集中在计算机相关的类别，而不是物理或天文类的论文。  
> 在系统里，一方面可以通过简单规则或小模型，用 `predict_query_category` 粗略判断 query 应该属于哪个大类；另一方面，通过 `_apply_category_logic` 对命中文档进行过滤或加权：  
> - 可以只保留类别前缀在允许列表中的文档；  
> - 也可以对类别匹配的文档增加一个分数加成，让它们更容易排在前面。
>
> 第六个关键方法是基于 SLA 的策略选择。  
> 在工程实践中，不同使用场景对延迟的容忍度不同，比如有的场景要求在 300ms 内返回答案，有的场景可以接受 1～2 秒的延迟来换取更好的检索和重排效果。  
> 在 `heuristics.choose_strategy` 中，我根据 `latency_budget_ms` 给出一个策略配置，比如：  
> - 预算非常紧时，只用 BM25 通路，关闭 Query Expansion 和 Rerank；  
> - 预算中等时，使用 Hybrid 检索并启用 PRF；  
> - 预算宽松时，则可以打开 Rerank 和 LLM 改写，让系统尽量追求效果。
>
> 在评测阶段，可以针对不同的预算设定，分别调用对应的策略来跑 `eval.py`，观察 Recall、MRR 和 end-to-end 延迟的变化，从而展示系统在“效果”和“速度”之间做平衡的能力。

---

## Slide 9｜实验与消融结果（关键表格）

这一页需要等你实际完成实验后再填具体数字，讲稿分两部分：**怎么分析数据** 和 **讲稿模版**。

### 9.1 实验数据应该怎么分析（操作建议）

1. **准备不同配置下的 CSV 文件**  
   - baseline：所有增强模块关闭的 Hybrid 检索结果，例如 `metrics_baseline_hybrid.csv`；  
   - dynamic α：只打开动态权重的结果，例如 `metrics_dynamic_alpha.csv`；  
   - rerank：在 Hybrid 基础上打开 Rerank 的结果，例如 `metrics_rerank.csv`；  
   - QE(PRF)：在 Hybrid 基础上打开 PRF 查询扩展的结果，例如 `metrics_expansion_prf.csv`；  
   - evidence：打开 evidence snippets 的结果，例如 `metrics_evidence.csv`；  
   - SLA：在不同 `latency_budget_ms` 下的结果，例如 `metrics_sla_300.csv / 800.csv / 1500.csv`。
2. **整理一个对比表格**  
   - 对每个 setting，从对应 CSV 中读出 `recall`, `mrr`, `search_ms`, `end2end_ms`；  
   - 放在一张表里，行是 setting，列是这几个指标；  
   - 可以在 Excel 或 pandas 里完成。
3. **确定要突出的“趋势”**  
   - 动态 α：整体或某部分 query 上 Recall/MRR 是否略有提升，延迟基本不变；  
   - Rerank：MRR 是否明显提升，search_ms 是否增加可接受的范围；  
   - QE：Recall@10 是否有显著提升，延迟增加多少；  
   - Evidence：指标可能变化不大，但配合前面展示的 evidence 示例，强调解释性提升；  
   - SLA：预算不同下，效果与 end2end_ms 的变化情况。
4. **决定要不要引用具体数值**  
   - 推荐方式：选几组最有代表性的数字，说“提升了大约 X 个百分点”；  
   - 其余地方用“略高”“明显提升”“增幅有限”这类词描述趋势即可。

### 9.2 实验页讲稿模版（填完数据后的思路）

> 这页展示的是几组典型配置的对比结果，为了便于阅读，我把不同策略下的 Recall、MRR 和延迟整理在一张表里。
>
> 最上面一行是基线配置，也就是只使用 Hybrid 检索，不启用动态 α、重排、查询扩展和类别/SLA 等增强功能。  
> 可以看到，在这种配置下，Recall@5 大约是 **R0**，MRR 大约是 **M0**，检索时间和端到端时间分别在 **S0 ms** 和 **E0 ms** 左右。
>
> 当我们打开 **动态混合权重** 时，整体 Recall 和 MRR 有一个小幅的提升，比如 Recall@5 从 **R0** 提升到 **R1**，MRR 也有类似的改善，而检索时间几乎不变。  
> 这说明仅靠轻量的 query 特征分析和自适应 α，就能让 Hybrid 对不同类型的问题更友好。
>
> 进一步打开 **Cross-Encoder Rerank** 后，可以看到 MRR 有更明显的提升，比如从 **M1** 升到 **M2**，说明前几名的排序质量明显变好；  
> 同时 search_ms 和 end2end_ms 有一定增加，在表里大约从 **S1** 增加到 **S2**，这是用更强模型“认真重排”的必然代价。
>
> 再看 **查询扩展**，尤其是 PRF，我们可以重点观察 Recall@10：  
> 在启用扩展后，很多原本召回不到的样本被找到了，Recall@10 有比较明显的提升；  
> 另一方面，检索时间也有所增加，因为系统需要对多个 query 变体进行并行检索。  
> 这部分在实际部署时，就可以根据 SLA 的预算决定要不要启用。
>
> 对于 **证据片段高亮**，表中的指标变化可能不算大，但它最大的价值在于解释性：  
> 前几页我们已经看到，它可以明确给出“哪篇论文的哪句话”作为证据，这在学术问答场景中是非常重要的。
>
> 最后，**SLA 策略** 让系统能够在不同延迟预算下自动选择配置：  
> 比如在 300ms 的预算下，可能只能使用简单的 BM25 或 Hybrid；而在 1500ms 的预算下，就可以打开 Rerank 和 Query Expansion，从而在允许的时间内取得最好的检索和生成效果。
>
> 综合来看，这些实验表明：每个增强模块都在 Recall、MRR 或解释性方面带来了可观的收益，而延迟的代价是可度量、可控制的，系统设计达到了“可调节、可组合”的目标。

---

## Slide 10｜工程实践与整体总结

**讲稿示例**

> 最后这一页，我从工程实现和方法设计两个角度做一个简要的总结。
>
> 在工程实现方面，这个系统从一个 4.6GB 的 arXiv metadata 文件出发，通过流式预处理、双通路索引、配置化的 pipeline，搭建起了一个完整、可运行的学术 RAG 问答系统。  
> 在此基础上，又通过六个模块把这套系统扩展成了一个“多策略可调”的实验平台：  
> 查询特征分析、动态权重、重排、扩展、证据抽取、类别过滤和 SLA 决策，都被封装成独立且可配置的组件，方便后续在不同数据集和模型上复用。
>
> 在方法设计方面，可以用三个关键词来概括这次工作：  
> 第一个关键词是 **自适应 Hybrid 检索**，包括动态 α 和查询扩展，让系统可以根据问题本身自动调整检索策略；  
> 第二个关键词是 **排序与解释增强**，通过 Cross-Encoder 重排提升前几名排序质量，通过句级 evidence 高亮提升答案的可解释性；  
> 第三个关键词是 **工程可控性**，通过类别感知和 SLA 策略，让系统能够在“准确性、延迟和领域相关性”之间做出清晰的权衡。
>
> 接下来如果继续深入，我希望在三个方向上扩展：  
> 一是尝试更强的 embedding 模型和重排模型，进一步提升检索效果；  
> 二是构建一个人工标注的高质量 QA 数据集，与合成 QA 做系统对比；  
> 三是把当前的命令行 Demo 封装成一个可视化的 Web 系统，让同学可以在线调整各种策略开关，直观看到 RAG 系统的行为变化。
>
> 我的汇报到这里就结束了，感谢各位老师和同学的聆听，也非常欢迎后面提问和交流。

---
```